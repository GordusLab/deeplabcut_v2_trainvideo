{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9308e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpiderCroppedVideoReader():\n",
    "    \"\"\" This class mimics OpenCV's VideoCapture class, but instead reads\n",
    "        the *.ufmf files, which contain 200x200 cropped/rotated and monochrome\n",
    "        spider images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, progressCallback=None):\n",
    "        self.fname = filename\n",
    "        self.iframe = 0\n",
    "        self.progressCallback = progressCallback\n",
    "\n",
    "        # DEBUG\n",
    "        self.MAX_NUM_FRAMES = 50 * 3600 * 24 * 100\n",
    "\n",
    "        # Is this a supported input file?\n",
    "        if not self.fname.endswith('.ufmf'):\n",
    "            raise Exception('Unsupported input file. Only *.ufmf files allowed.')\n",
    "\n",
    "        # Open file to determine size\n",
    "        from motmot.SpiderMovie import SpiderMovie\n",
    "        mov = SpiderMovie(filename)\n",
    "        #self.arr = np.memmap(self.fname, dtype=np.uint8, mode='r')\n",
    "        #N = self.arr.shape[0]\n",
    "        N = mov.shape[0]*mov.shape[1]*mov.shape[2]\n",
    "        #del self.arr\n",
    "        # Ensure this file has the right size\n",
    "        if N % (1024 * 1024) != 0:\n",
    "            raise Exception('Image does not have expected size of 1024x1024.')\n",
    "        # Open again with right shape\n",
    "        #self.arr = np.memmap(self.fname, dtype=np.uint8, mode='r', shape=(int(N / (1024 * 1024)), 1024, 1024))\n",
    "        self.matrix_shape = mov.shape\n",
    "        self.mov = mov\n",
    "\n",
    "    def get_bbox(self):\n",
    "\n",
    "        return (0, self.mov.shape[1], 0, self.mov.shape[2])\n",
    "        \n",
    "    # Get various metadata\n",
    "    def get(self, i):\n",
    "        # Call progress callback\n",
    "        if self.progressCallback is not None:\n",
    "            try:\n",
    "                self.progressCallback(float(i) / self.matrix_shape[0])\n",
    "            except:\n",
    "                pass\n",
    "        # NUM. FRAMES\n",
    "        if i == 7:\n",
    "            if self.matrix_shape is not None:\n",
    "                return min(self.MAX_NUM_FRAMES, self.matrix_shape[0])\n",
    "            else:\n",
    "                raise Exception('File closed.')\n",
    "        # FPS\n",
    "        elif i == 5:\n",
    "            return 50\n",
    "        # HEIGHT\n",
    "        elif i == 4:\n",
    "            return 1024\n",
    "        # WIDTH\n",
    "        elif i == 3:\n",
    "            return 1024\n",
    "        # ERROR\n",
    "        else:\n",
    "            raise Exception('Unsupported metadata requested: {}'.format(i))\n",
    "\n",
    "    def isOpened(self):\n",
    "        return True\n",
    "\n",
    "    def read(self):\n",
    "        import numpy as np\n",
    "        if self.iframe >= self.get(7):\n",
    "            return False, np.zeros((1024, 1024), dtype=np.uint8)\n",
    "        else:\n",
    "            self.iframe += 1\n",
    "            return True, self.mov[self.iframe - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb543fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z:/HsinYi/Test Video From Darya/test_clip_dlc_v2/raw\\\\bias_video_cam_2_date_2023_05_31_time_19_56_23_v001.all.s.ufmf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direcory = 'Z:/HsinYi/Test Video From Darya/'\n",
    "project_folder = 'test_clip_dlc_v2/'\n",
    "basepath = direcory + project_folder\n",
    "project_name = 'TEST'\n",
    "scorer = 'PC'\n",
    "\n",
    "import os, glob\n",
    "\n",
    "\n",
    "video_list = glob.glob(os.path.join(basepath, 'raw/')+'*.s.ufmf')\n",
    "video_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d14a65-0af8-481f-bae2-b5b7d382dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023-11-22'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "from deeplabcut.utils import auxiliaryfunctions\n",
    "\n",
    "months_3letter = {\n",
    "        1: \"Jan\",\n",
    "        2: \"Feb\",\n",
    "        3: \"Mar\",\n",
    "        4: \"Apr\",\n",
    "        5: \"May\",\n",
    "        6: \"Jun\",\n",
    "        7: \"Jul\",\n",
    "        8: \"Aug\",\n",
    "        9: \"Sep\",\n",
    "        10: \"Oct\",\n",
    "        11: \"Nov\",\n",
    "        12: \"Dec\",\n",
    "    }\n",
    "date = dt.today()\n",
    "month = months_3letter[date.month]\n",
    "day = date.day\n",
    "d = str(month[0:3] + str(day))\n",
    "date = dt.today().strftime(\"%Y-%m-%d\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd1b6c8-40d2-47fe-9342-642aa96d5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"{pn}-{exp}-{date}\".format(pn=project_name, exp=scorer, date=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7923636c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n",
      "Created \"\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\videos\"\n",
      "Created \"\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\labeled-data\"\n",
      "Created \"\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\training-datasets\"\n",
      "Created \"\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\dlc-models\"\n",
      "Copying the videos\n",
      "\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\videos\\bias_video_cam_2_date_2023_05_31_time_19_56_23_v001.all.s.ufmf\n",
      "Generated \"\\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2\\TEST-PC-2023-11-22\\config.yaml\"\n",
      "\n",
      "A new project with name TEST-PC-2023-11-22 is created at \\\\10.99.66.32\\Team Spider\\HsinYi\\Test Video From Darya\\test_clip_dlc_v2 and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\napari_dlc_trainvideo\\Lib\\site-packages\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\10.99.66.32\\\\Team Spider\\\\HsinYi\\\\Test Video From Darya\\\\test_clip_dlc_v2\\\\TEST-PC-2023-11-22\\\\config.yaml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.create_new_project(project_name , scorer, video_list, working_directory=basepath, copy_videos= True, videoReader = SpiderCroppedVideoReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742bc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = basepath+ '/'+project+'/config.yaml'\n",
    "cfg = deeplabcut.auxiliaryfunctions.read_config(config_path)\n",
    "cfg['bodyparts'] =['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n",
    "deeplabcut.auxiliaryfunctions.write_config(config_path, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d475914-8b23-4202-968e-1fb4c3083e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bias_video_cam_2_date_2023_05_31_time_19_56_23_v001.all.s.ufmf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoname = video_list[0].split('raw\\\\')[1]\n",
    "videoname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643b5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING-SOME LABELS FOR THE FRAMES\n"
     ]
    }
   ],
   "source": [
    "## Skip extract frames\n",
    "## Skip label frames\n",
    "\n",
    "## Generate labeling csv\n",
    "import os, glob\n",
    "\n",
    "videoname = video_list[0].split('raw\\\\')[1]\n",
    "joint_filename = basepath + '/croprot/' +videoname.replace('.ufmf','_dlc_abs.npy')\n",
    "\n",
    "from deeplabcut.generate_training_dataset import creating_labeling_csv\n",
    "creating_labeling_csv.creating_labeling_csv(config_path, basepath, videoname, joint_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab65e0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\napari_dlc_trainvideo\\Lib\\site-packages\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by PC.\n",
      "This is ufmf video. Randomly selected 25 images to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.check_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89342412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING TRAININGSET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/28914 [00:00<?, ?it/s]C:\\Users\\Gordus_Lab\\anaconda3\\envs\\napari_dlc_trainvideo\\Lib\\site-packages\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 28914/28914 [08:57<00:00, 53.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([19736, 20841, 22215, ...,  4317,  2514,  4756]),\n",
       "   array([ 4996,  7622,  6876, ..., 14816,  2448, 23582])))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplabcut\n",
    "config_path = basepath+ '/'+project+'/config.yaml'\n",
    "cfg = deeplabcut.auxiliaryfunctions.read_config(config_path)\n",
    "print(\"CREATING TRAININGSET\")\n",
    "deeplabcut.create_training_dataset(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3478912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\config.py:43: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n",
      "Config:\n",
      "{'all_joints': [[0],\n",
      "                [1],\n",
      "                [2],\n",
      "                [3],\n",
      "                [4],\n",
      "                [5],\n",
      "                [6],\n",
      "                [7],\n",
      "                [8],\n",
      "                [9],\n",
      "                [10],\n",
      "                [11],\n",
      "                [12],\n",
      "                [13],\n",
      "                [14],\n",
      "                [15],\n",
      "                [16],\n",
      "                [17],\n",
      "                [18],\n",
      "                [19],\n",
      "                [20],\n",
      "                [21],\n",
      "                [22],\n",
      "                [23],\n",
      "                [24],\n",
      "                [25]],\n",
      " 'all_joints_names': ['0',\n",
      "                      '1',\n",
      "                      '2',\n",
      "                      '3',\n",
      "                      '4',\n",
      "                      '5',\n",
      "                      '6',\n",
      "                      '7',\n",
      "                      '8',\n",
      "                      '9',\n",
      "                      '10',\n",
      "                      '11',\n",
      "                      '12',\n",
      "                      '13',\n",
      "                      '14',\n",
      "                      '15',\n",
      "                      '16',\n",
      "                      '17',\n",
      "                      '18',\n",
      "                      '19',\n",
      "                      '20',\n",
      "                      '21',\n",
      "                      '22',\n",
      "                      '23',\n",
      "                      '24',\n",
      "                      '25'],\n",
      " 'batch_size': 1,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_TESTSep28\\\\TEST_PC95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\Gordus_Lab\\\\anaconda3\\\\envs\\\\dlc-windowsCPU-trainvideo\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_TESTSep28\\\\Documentation_data-TEST_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 26,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'Z:/HsinYi/Test Video From Darya/DLC '\n",
      "                 'retrain//TEST-PC-2023-09-28',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 1000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'Z:\\\\HsinYi\\\\Test Video From Darya\\\\DLC '\n",
      "                    'retrain\\\\TEST-PC-2023-09-28\\\\dlc-models\\\\iteration-0\\\\TESTSep28-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'use_gt_segm': False,\n",
      " 'video': False,\n",
      " 'video_batch': False,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gordus_lab\\documents\\repositories\\deeplabcut\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'Z:\\\\HsinYi\\\\Test Video From Darya\\\\DLC retrain\\\\TEST-PC-2023-09-28\\\\dlc-models\\\\iteration-0\\\\TESTSep28-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'dataset_type': 'default', 'use_gt_segm': False, 'batch_size': 1, 'video': False, 'video_batch': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25]], 'all_joints_names': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25'], 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_TESTSep28\\\\TEST_PC95shuffle1.mat', 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\Gordus_Lab\\\\anaconda3\\\\envs\\\\dlc-windowsCPU-trainvideo\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_TESTSep28\\\\Documentation_data-TEST_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1000]], 'net_type': 'resnet_50', 'num_joints': 26, 'pos_dist_thresh': 17, 'project_path': 'Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28', 'save_iters': 1000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1000/1001 [56:23<00:02,  2.67s/it]iteration: 1000 loss: 0.0340 lr: 0.005\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1001/1001 [56:26<00:00,  3.38s/it]\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}} = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 54, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:40)  = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "Caused by op 'fifo_queue_enqueue', defined at:\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 538, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-197dd40c7d57>\", line 6, in <module>\n",
      "    deeplabcut.train_network(config_path)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 79, in train_network\n",
      "    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep) #pass on path and file name for pose_cfg.yaml!\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 89, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 40, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 341, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 4380, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:40)  = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Here I modify the config file to train only 1000 step to make sure there is no bug in the code. By default, the training step is 1030000\n",
    "\n",
    "print('Train the network')\n",
    "deeplabcut.train_network(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfbd2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28/evaluation-results/  already exists!\n",
      "Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28\\evaluation-results\\iteration-0\\TESTSep28-trainset95shuffle1  already exists!\n",
      "Running  DeepCut_resnet50_TESTSep28shuffle1_1000  with # of trainingiterations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\config.py:43: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This net has already been evaluated!\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deeplabcut.evaluate_network(\n",
    "    config_path,\n",
    "    plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6994d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\config.py:43: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1000 for model Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28\\dlc-models\\iteration-0\\TESTSep28-trainset95shuffle1\n",
      "INFO:tensorflow:Restoring parameters from Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28\\dlc-models\\iteration-0\\TESTSep28-trainset95shuffle1\\train\\snapshot-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28\\dlc-models\\iteration-0\\TESTSep28-trainset95shuffle1\\train\\snapshot-1000\n",
      "c:\\users\\gordus_lab\\documents\\repositories\\deeplabcut\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28/videos/bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf\n",
      "Loading  Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28/videos/bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf\n",
      "Duration of video [s]:  60.02 , recorded with  50 fps!\n",
      "Overall # of frames:  3001  found with (before cropping) frame dimensions:  1024 1024\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3030it [1:42:01,  2.06s/it]                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3030it [1:42:09,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in Z:\\HsinYi\\Test Video From Darya\\DLC retrain\\TEST-PC-2023-09-28\\videos...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "basepath = 'Z:/HsinYi/Test Video From Darya/DLC retrain/'\n",
    "config_path = basepath+ '/TEST-PC-2023-09-28/config.yaml'\n",
    "\n",
    "deeplabcut.analyze_videos(\n",
    "    config_path, videos=[basepath+'/TEST-PC-2023-09-28/videos/'+'bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf'], videoReader = SpiderCroppedVideoReader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93af68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Gordus_Lab\\anaconda3\\envs\\dlc-windowsCPU-trainvideo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\gordus_lab\\documents\\repositories\\deeplabcut\\deeplabcut\\motmot\\ufmf\\ufmf.py:199: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  assert key not in value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting %  Z:\\HsinYi\\Test Video From Darya\\DLC retrain\\TEST-PC-2023-09-28\\videos ['Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28/videos/bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf']\n",
      "Loading  Z:/HsinYi/Test Video From Darya/DLC retrain//TEST-PC-2023-09-28/videos/bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf and data.\n",
      "False 0 1024 0 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:29<00:00, 102.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing video...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "basepath = 'Z:/HsinYi/Test Video From Darya/DLC retrain/'\n",
    "config_path = basepath+ '/TEST-PC-2023-09-28/config.yaml'\n",
    "\n",
    "## Note: Since I only train 1000 step, the video should look very bad.\n",
    "deeplabcut.create_labeled_ufmfvideo(config_path,[basepath+'/TEST-PC-2023-09-28/videos/'+'bias_video_cam_2_date_2023_09_26_time_20_44_05_v001.all.s.ufmf'],videotype='ufmf',codec='mp4v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974e63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5cf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:napari_dlc_trainvideo]",
   "language": "python",
   "name": "conda-env-napari_dlc_trainvideo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
